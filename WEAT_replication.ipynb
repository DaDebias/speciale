{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant packages\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import scipy\n",
    "import spacy\n",
    "import gensim\n",
    "import danlp\n",
    "import sys\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v1, v2, embedding):\n",
    "    \"\"\"\n",
    "    Returns cosine of the angle between two vectors and automatically normalizes for vector length\n",
    "    The function first loads the embeddings for the two words v1 and v2 that the cosine similarity has to be calculated for. \n",
    "    It then calculates the cosine similarity using the formula: cos(x, y) = x . y / ||X|| * ||y||\n",
    "    np.dot returns the dot product of x and y (x . y)\n",
    "    np.linalg.norm returns the length of the given vector (||x|| and ||y||)\n",
    "    \n",
    "    \"\"\"\n",
    "    v1 = embedding[v1]\n",
    "    v2 = embedding[v2]\n",
    "    cos = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weat_association(W, A, B, embedding):\n",
    "    \"\"\"\n",
    "    The function makes two lists - one that stores the cosine distance between a target word W and all of attribute words in A and one storing the cosine distance between the target word W and all attribute words in B.\n",
    "    Cosine distance is calculated by the function defined as cosine_sim \n",
    "    The function returns the difference in means between the word W's association with respectively A and B\n",
    "    Returns what corresponds to s(w, A, B) in the equations defined in the thesis\n",
    "\n",
    "    \"\"\"\n",
    "    cosine_scoresA = [] #create an empty list for associations between W and words in A\n",
    "    cosine_scoresB = [] #create an empty list for associations between W and words in B\n",
    "    for i in A:\n",
    "        cosine_scoresA.append(cosine_sim(W, i, embedding))\n",
    "    for i in B:\n",
    "        cosine_scoresB.append(cosine_sim(W, i, embedding))\n",
    "        \n",
    "    association = np.mean(cosine_scoresA) - np.mean(cosine_scoresB)\n",
    "    \n",
    "    return association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weat_differential_association(X, Y, A, B, embedding):\n",
    "    \"\"\"\n",
    "    The function calculates the association of all words W in respectively X and Y and there association with A and B\n",
    "    It uses the function defined above (weat_association) to calculate the different in association between a given \n",
    "    word W's association with words in respectively A and B - but it iterates for all words W in respectively X and Y\n",
    "    Returns what corresponds to s(X, Y, A, B) in the equations\n",
    "    \"\"\"\n",
    "    associationX = []\n",
    "    associationY = []\n",
    "    for i in X:\n",
    "        associationX.append(weat_association(i, A, B, embedding))\n",
    "    for i in Y:\n",
    "        associationY.append(weat_association(i, A, B, embedding))\n",
    "    \n",
    "    diff_association = np.sum(associationX) - np.sum(associationY)\n",
    "    \n",
    "    return diff_association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weat_effect_size(X, Y, A, B,embedding):\n",
    "    \"\"\"\n",
    "    We first need to retrieve s(w, A, B) for all words w (i) in both X and Y and then find the standard deviation of all words\n",
    "    We use the previously defined weat_association to retrieve s(w, A, B)\n",
    "    We also retrieve the mean difference in association between the target words' association to X and Y\n",
    "    \"\"\"\n",
    "      \n",
    "    associationX = []\n",
    "    associationY = []\n",
    "    for i in X:\n",
    "        associationX.append(weat_association(i, A, B, embedding))\n",
    "    for i in Y:\n",
    "        associationY.append(weat_association(i, A, B, embedding))\n",
    "    associationXY = []\n",
    "    associationXY = associationX + associationY\n",
    "    tmp1 = np.mean(associationX) - np.mean(associationY)\n",
    "    tmp2 = np.std(associationXY, ddof=1) \n",
    "        \n",
    "    effect_size = tmp1/tmp2\n",
    "    \n",
    "    return effect_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permutation test p-value - has to be adjusted + code has to run more smoothly\n",
    "from scipy import stats\n",
    "\n",
    "def weat_p_value(X, Y, A, B, embedding, p):\n",
    "    \"\"\"\n",
    "    Returns one-sided p-value of the permutation test \n",
    "    What the permutation test basically does: we want to make a lot (i) possible combinations of our target words and assess\n",
    "    their association to the attributes. We then calculate the proportion of differential association that are higher for \n",
    "    permuted distributions than for the non permuted distrubition\n",
    "\n",
    "    \"\"\"\n",
    "    diff_association = weat_differential_association(X, Y, A, B, embedding)\n",
    "    target_words = np.concatenate((X, Y), axis=0)\n",
    "    np.random.shuffle(target_words) #shuffle target words before permutations\n",
    "    \n",
    "    #Test if target words can be divided into two sets of equal size - otherwise print warning\n",
    "    if target_words.shape[0] % 2 != 0:\n",
    "        print('WARNING - target word set can not be divided into two sets of equal size')\n",
    "        \n",
    "    partition_diff_association = [] #Create empty list to be filled during loop\n",
    "\n",
    "    for i in range(p): #Iterate p times (number of permutations)\n",
    "        seq = np.random.permutation(target_words) #Permute target words\n",
    "        partition_X = seq[:len(seq)//2] #Load  first partition of data to create to sets of permuted target words\n",
    "        partition_Y = seq[len(seq)//2:] #Load second partition of data to create to sets of permuted target words\n",
    "        #Calculate and append differential association for permuted target words to attributes\n",
    "        partition_diff_association.append(weat_differential_association(partition_X, partition_Y, A, B, embedding))\n",
    "      \n",
    "    partition_diff_association = np.array(partition_diff_association) #Convert differential association for all permuted samples to numpy array\n",
    "\n",
    "    mean = np.mean(partition_diff_association) #Mean differential association for permutations\n",
    "    stdev = np.std(partition_diff_association) #Standard deviation of differential association for permutations\n",
    "    pvalue = ((np.sum(partition_diff_association > diff_association)) / (len(partition_diff_association))) #Calculation of p-value, corresponds to proportion of differential association for permuted target words that are higher than for the non permuted value\n",
    "#OBS p-value not working, tæller kan aldrig være større end nævner\n",
    "    return diff_association, mean, stdev, pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we make one function that returns all relevant values for one embedding and one kind of gender bias\n",
    "\n",
    "def results_weat(X, Y, A, B, embedding, p):\n",
    "    #Define empty lists\n",
    "    diff_association = []\n",
    "    effect_size = []\n",
    "    mean_diff_association_p = []\n",
    "    std_diff_association_p = []\n",
    "    pvalue = []\n",
    "\n",
    "    #Retrieve values from already defined functions\n",
    "    diff_association = weat_differential_association(X, Y, A, B, embedding)\n",
    "    effect_size = weat_effect_size(X, Y, A, B,embedding)\n",
    "    pvalue = weat_p_value(X, Y, A, B, embedding, p)[3]\n",
    "    \n",
    "    #Combine all values in dataframe\n",
    "    s1=pd.Series(diff_association,name='diff_association')\n",
    "    s2=pd.Series(effect_size,name='effect_size')\n",
    "    s3=pd.Series(pvalue ,name='pvalue')\n",
    "\n",
    "    results = pd.concat([s1,s2,s3], axis=1)\n",
    "        \n",
    "    #Return dataframe with all results\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caliskan et. al algorithms and use of words\n",
    "#p-value can differ because it is measured by a permutation test, but the rest of the results should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2196017, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.840B.300d.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_association</th>\n",
       "      <th>effect_size</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.345604</td>\n",
       "      <td>1.237453</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diff_association  effect_size  pvalue\n",
       "0          0.345604     1.237453   0.004"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set permutations\n",
    "p =1000\n",
    "\n",
    "#Set target and attribute words - Science vs. Arts\n",
    "A = ['brother', 'father', 'uncle', 'grandfather', 'son', 'he', 'his', 'him']\n",
    "B = ['sister', 'mother', 'aunt', 'grandmother', 'daughter', 'she', 'hers', 'her']\n",
    "X = ['science', 'technology', 'physics', 'chemistry', 'Einstein', 'NASA', 'experiment', 'astronomy']\n",
    "Y = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n",
    "   \n",
    "results_weat(X, Y, A, B, glove_model, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google News replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from GoogleNews-vectors-negative300.bin\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': 'GoogleNews-vectors-negative300.bin', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'ignore_ext': False, 'transport_params': None}\n",
      "INFO:gensim.models.utils_any2vec:loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "# Load Google's pre-trained Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_association</th>\n",
       "      <th>effect_size</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.357187</td>\n",
       "      <td>1.243855</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diff_association  effect_size  pvalue\n",
       "0          0.357187     1.243855   0.003"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set permutations\n",
    "p =1000\n",
    "\n",
    "#Set target and attribute words - Science vs. Arts\n",
    "A = ['brother', 'father', 'uncle', 'grandfather', 'son', 'he', 'his', 'him']\n",
    "B = ['sister', 'mother', 'aunt', 'grandmother', 'daughter', 'she', 'hers', 'her']\n",
    "X = ['science', 'technology', 'physics', 'chemistry', 'Einstein', 'NASA', 'experiment', 'astronomy']\n",
    "Y = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n",
    "   \n",
    "results_weat(X, Y, A, B, word2vec_model, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
